<!--
  ~ Smart Data Lake - Build your data lake the smart way.
  ~
  ~ Copyright Â© 2019-2020 ELCA Informatique SA (<https://www.elca.ch>)
  ~
  ~ This program is free software: you can redistribute it and/or modify
  ~ it under the terms of the GNU General Public License as published by
  ~ the Free Software Foundation, either version 3 of the License, or
  ~ (at your option) any later version.
  ~
  ~ This program is distributed in the hope that it will be useful,
  ~ but WITHOUT ANY WARRANTY; without even the implied warranty of
  ~ MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  ~ GNU General Public License for more details.
  ~
  ~ You should have received a copy of the GNU General Public License
  ~ along with this program. If not, see <http://www.gnu.org/licenses/>.
  -->
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
	<modelVersion>4.0.0</modelVersion>

	<groupId>io.smartdatalake</groupId>
	<artifactId>smartdatalake_${scala.minor.version}</artifactId>
	<version>1.1.1-SNAPSHOT</version>
	<packaging>jar</packaging>

	<licenses>
		<license>
			<name>GNU General Public License (GPL) version 3</name>
			<url>https://www.gnu.org/licenses/gpl-3.0.html</url>
		</license>
	</licenses>

	<!-- Used for license header by maven-license-plugin -->
	<name>Smart Data Lake</name>
	<description>Build your data lake the smart way.</description>
	<url>http://www.smartdatalake.io</url>
	<inceptionYear>2019</inceptionYear>
	<organization>
		<name>ELCA Informatique SA</name>
		<url>https://www.elca.ch</url>
	</organization>

	<developers>
		<developer>
			<name>Smart Data Lake</name>
			<email>smartdatalake@elca.ch</email>
			<organization>ELCA Informatik AG</organization>
			<organizationUrl>http://www.elca.ch</organizationUrl>
		</developer>
	</developers>

	<profiles>
		<profile>
			<id>fat-jar</id>
			<properties>
				<skip.assembly>false</skip.assembly>
				<scala.deps.scope>provided</scala.deps.scope>
				<spark.deps.scope>provided</spark.deps.scope>
			</properties>
		</profile>
		<profile>
			<id>fat-jar-with-spark</id>
			<properties>
				<skip.assembly>false</skip.assembly>
				<scala.deps.scope>compile</scala.deps.scope>
				<spark.deps.scope>compile</spark.deps.scope>
			</properties>
		</profile>
		<profile>
			<!-- do not use this profile if you want to see warnings about missing links
            it makes sense to check warnings about internal (public) members, but we will always have
            warnings for external libraries which is why we deactivate them -->
			<id>scala-doc-nolinkwarnings</id>
			<properties>
				<noLinkWarnings>-no-link-warnings</noLinkWarnings>
			</properties>
		</profile>
		<profile>
			<id>scala-2.12</id>
			<properties>
				<scala.minor.version>2.12</scala.minor.version>
				<scala.version>${scala.minor.version}.10</scala.version>
			</properties>
		</profile>
		<profile>
			<id>scala-2.11</id>
			<activation><activeByDefault>true</activeByDefault></activation>
			<properties>
				<scala.minor.version>2.11</scala.minor.version>
				<scala.version>${scala.minor.version}.12</scala.version>
			</properties>
		</profile>
	</profiles>

	<properties>
		<project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
		<skip.assembly>true</skip.assembly>
		<noLinkWarnings>-unchecked</noLinkWarnings>

		<maven.compiler.source>1.8</maven.compiler.source>
		<maven.compiler.target>1.8</maven.compiler.target>
		<maven.compiler.release>8</maven.compiler.release>

		<scala.deps.scope>compile</scala.deps.scope>
		<spark.deps.scope>compile</spark.deps.scope>

		<spark.version>2.4.4</spark.version>

		<!-- SPARK-PARENT start: properties copied from spark-parent pom version 2.4.4 -->
		<!-- attention - manually adapted are: zookeeper, scala.version, scala.binary.version -->
		<slf4j.version>1.7.16</slf4j.version>
		<log4j.version>1.2.17</log4j.version>
		<!-- hadoop version must match the hadoop version included by spark-core -->
		<!-- could be better to use cloudera version of hadoop because they backported many bugfixes from later versions -->
		<!--hadoop.version>2.6.0-cdh5.16.1</hadoop.version-->
		<hadoop.version>2.6.5</hadoop.version>
		<protobuf.version>2.5.0</protobuf.version>
		<yarn.version>${hadoop.version}</yarn.version>
		<flume.version>1.6.0</flume.version>
		<zookeeper.version>3.4.13</zookeeper.version><!--Overrides version 3.4.6 in spark core/hadoop in order to support kafka tests-->
		<curator.version>2.6.0</curator.version>
		<hive.group>org.spark-project.hive</hive.group>
		<!--  Version used in Maven Hive dependency  -->
		<hive.version>1.2.1.spark2</hive.version>
		<!--  Version used for internal directory structure  -->
		<hive.version.short>1.2.1</hive.version.short>
		<derby.version>10.12.1.1</derby.version>
		<parquet.version>1.10.1</parquet.version>
		<orc.version>1.5.5</orc.version>
		<orc.classifier>nohive</orc.classifier>
		<hive.parquet.version>1.6.0</hive.parquet.version>
		<jetty.version>9.3.24.v20180605</jetty.version>
		<javaxservlet.version>3.1.0</javaxservlet.version>
		<chill.version>0.9.3</chill.version>
		<ivy.version>2.4.0</ivy.version>
		<oro.version>2.0.8</oro.version>
		<codahale.metrics.version>3.1.5</codahale.metrics.version>
		<avro.version>1.8.2</avro.version>
		<avro.mapred.classifier>hadoop2</avro.mapred.classifier>
		<aws.kinesis.client.version>1.8.10</aws.kinesis.client.version>
		<!--  Should be consistent with Kinesis client dependency  -->
		<aws.java.sdk.version>1.11.271</aws.java.sdk.version>
		<!--  the producer is used in tests  -->
		<aws.kinesis.producer.version>0.12.8</aws.kinesis.producer.version>
		<!--   org.apache.httpcomponents/httpclient -->
		<commons.httpclient.version>4.5.6</commons.httpclient.version>
		<commons.httpcore.version>4.4.10</commons.httpcore.version>
		<!--   commons-httpclient/commons-httpclient -->
		<httpclient.classic.version>3.1</httpclient.classic.version>
		<commons.math3.version>3.4.1</commons.math3.version>
		<!--  managed up from 3.2.1 for SPARK-11652  -->
		<commons.collections.version>3.2.2</commons.collections.version>
		<scala.version>${scala.version}</scala.version>
		<scala.binary.version>${scala.minor.version}</scala.binary.version>
		<codehaus.jackson.version>1.9.13</codehaus.jackson.version>
		<fasterxml.jackson.version>2.6.7</fasterxml.jackson.version>
		<fasterxml.jackson.databind.version>2.6.7.1</fasterxml.jackson.databind.version>
		<snappy.version>1.1.7.3</snappy.version>
		<netlib.java.version>1.1.2</netlib.java.version>
		<calcite.version>1.2.0-incubating</calcite.version>
		<commons-codec.version>1.10</commons-codec.version>
		<commons-io.version>2.4</commons-io.version>
		<!--  org.apache.commons/commons-lang/ -->
		<commons-lang2.version>2.6</commons-lang2.version>
		<!--  org.apache.commons/commons-lang3/ -->
		<commons-lang3.version>3.5</commons-lang3.version>
		<datanucleus-core.version>3.2.10</datanucleus-core.version>
		<janino.version>3.0.9</janino.version>
		<jersey.version>2.22.2</jersey.version>
		<joda.version>2.9.3</joda.version>
		<jodd.version>3.5.2</jodd.version>
		<jsr305.version>1.3.9</jsr305.version>
		<libthrift.version>0.9.3</libthrift.version>
		<antlr4.version>4.7</antlr4.version>
		<jpam.version>1.1</jpam.version>
		<selenium.version>2.52.0</selenium.version>
		<!--
            Managed up from older version from Avro; sync with jackson-module-paranamer dependency version
             -->
		<paranamer.version>2.8</paranamer.version>
		<maven-antrun.version>1.8</maven-antrun.version>
		<commons-crypto.version>1.0.0</commons-crypto.version>
		<!--
            If you are changing Arrow version specification, please check ./python/pyspark/sql/utils.py,
            ./python/run-tests.py and ./python/setup.py too.
             -->
		<arrow.version>0.10.0</arrow.version>
		<!-- SPARK-PARENT finished -->


		<!-- other dependency versions -->
		<scopt.version>3.7.1</scopt.version>
		<databricks.spark-xml.version>0.9.0</databricks.spark-xml.version>
		<spark.excel.version>0.12.0</spark.excel.version>
		<ucanaccess.version>4.0.4</ucanaccess.version>
		<!-- when changing config version, remember to update it in DatabricksSmartDataLakeBuilder main method -->
		<typesafe.config.version>1.3.4</typesafe.config.version>
		<monix.version>3.1.0</monix.version>
		<scala-arm.version>2.0</scala-arm.version>
		<splunk.version>1.6.5.0</splunk.version>
		<sshj.version>0.21.1</sshj.version>
		<kafka.version>2.4.1</kafka.version>
		<keycloak.version>4.5.0.Final</keycloak.version>
		<deltaTable.version>0.5.0</deltaTable.version>
		<poi.version>4.0.0</poi.version>

		<scalatest.test.version>3.0.1</scalatest.test.version>
		<sshd.test.version>2.3.0</sshd.test.version>

	</properties>

	<distributionManagement>
		<repository>
			<id>bintray-smart-data-lake-smart-data-lake</id>
			<name>smart-data-lake-smart-data-lake</name>
			<url>https://api.bintray.com/maven/smart-data-lake/smart-data-lake/smart-data-lake/;publish=1</url>
		</repository>
	</distributionManagement>

	<scm>
		<connection>scm:git:git://github.com/smart-data-lake/smart-data-lake.git</connection>
		<developerConnection>scm:git:ssh://github.com/smart-data-lake/smart-data-lake.git</developerConnection>
		<url>http://github.com/smart-data-lake/smart-data-lake/tree/master</url>
	</scm>

	<repositories>

		<repository>
			<id>spring-plugins</id>
			<name>Spring Plugins Repository</name>
			<url>https://repo.spring.io/plugins-release/</url>
		</repository>
		<repository>
			<id>confluent</id>
			<name>Confluent Schema  Repository</name>
			<url>https://packages.confluent.io/maven/</url>
		</repository>
	</repositories>

	<build>
		<!-- for creating scala docs for workflow objects only, temporarily replace the line below with: <sourceDirectory>src/main/scala/io/smartdatalake/workflow</sourceDirectory> -->
		<sourceDirectory>src/main/scala</sourceDirectory>
		<testSourceDirectory>src/test/scala</testSourceDirectory>
		<resources>
			<resource>
				<directory>src/main/resources</directory>
			</resource>
			<resource>
				<targetPath>META-INF</targetPath>
				<directory>${basedir}</directory>
				<filtering>false</filtering>
				<includes>
					<include>COPYING</include> <!-- the file containing the license text -->
				</includes>
			</resource>
		</resources>
		<testResources>
			<testResource>
				<directory>src/test/resources</directory>
			</testResource>
		</testResources>

		<plugins>
			<!-- Compiles Scala sources. -->
			<plugin>
				<groupId>net.alchim31.maven</groupId>
				<artifactId>scala-maven-plugin</artifactId>
				<version>4.3.1</version>
				<executions>
					<execution>
						<goals>
							<goal>compile</goal>
							<goal>testCompile</goal>
						</goals>
					</execution>
				</executions>
				<configuration>
					<scalaCompatVersion>${scala.minor.version}</scalaCompatVersion>
					<checkMultipleScalaVersions>true</checkMultipleScalaVersions>
					<failOnMultipleScalaVersions>true</failOnMultipleScalaVersions>
					<recompileMode>incremental</recompileMode>
					<args>
						<arg>-unchecked</arg>
						<arg>-deprecation</arg>
						<arg>-feature</arg>
						<arg>-explaintypes</arg>
						<arg>-target:jvm-1.8</arg>
						<arg>${noLinkWarnings}</arg>
					</args>
					<jvmArgs>
						<jvmArg>-Xms64m</jvmArg>
						<jvmArg>-Xmx1024m</jvmArg>
					</jvmArgs>
				</configuration>
			</plugin>

			<!-- rewrite pom for compiling with different scala version profiles -->
			<plugin>
				<groupId>org.spurint.maven.plugins</groupId>
				<artifactId>scala-cross-maven-plugin</artifactId>
				<version>0.2.1</version>
				<executions>
					<execution>
						<id>rewrite-pom</id>
						<goals>
							<goal>rewrite-pom</goal>
						</goals>
					</execution>
				</executions>
			</plugin>

			<!-- Copies files in resources folders to target folder. -->
			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-resources-plugin</artifactId>
				<version>3.1.0</version>
			</plugin>

			<!-- Checks whether source files have the specified license header. -->
			<plugin>
				<groupId>com.mycila</groupId>
				<artifactId>license-maven-plugin</artifactId>
				<version>3.0</version>
				<configuration>
					<header>src/license/gplv3-header.txt</header>
					<properties>
						<copyright.name>${project.organization.name}</copyright.name>
						<copyright.contact>${project.organization.url}</copyright.contact>
					</properties>
					<includes>
						<include>src/**</include>
					</includes>
					<excludes>
						<exclude>**/*.accdb</exclude>
						<exclude>**/*.mdb</exclude>
						<exclude>**/*.csv</exclude>
						<exclude>**/*.keytab</exclude>
						<exclude>**/*.pkcs12</exclude>
					</excludes>
					<mapping>
						<scala>SLASHSTAR_STYLE</scala>
						<conf>SCRIPT_STYLE</conf>
					</mapping>
					<failIfMissing>false</failIfMissing>
				</configuration>
				<dependencies>
					<dependency>
						<groupId>com.mycila</groupId>
						<artifactId>license-maven-plugin-git</artifactId>
						<version>3.0</version>
					</dependency>
				</dependencies>
				<executions>
					<execution>
						<!--<phase>process-sources</phase>-->
						<goals>
							<goal>check</goal>
						</goals>
					</execution>
				</executions>
			</plugin>

			<!-- Creates the jar without dependencies -->
			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-jar-plugin</artifactId>
				<version>3.1.2</version>
				<configuration>
					<archive>
						<manifest>
							<addClasspath>true</addClasspath>
							<mainClass>io.smartdatalake.app.LocalSmartDataLakeBuilder</mainClass>
						</manifest>
						<manifestEntries>
							<Implementation-Version>${project.version}</Implementation-Version>
						</manifestEntries>
					</archive>
					<excludes>
						<exclude>log4j.properties</exclude> <!-- Logging configuration should be left to user. -->
					</excludes>
				</configuration>
			</plugin>

			<!-- Creates a JAR file with the source files of the project. -->
			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-source-plugin</artifactId>
				<version>3.1.0</version>
				<executions>
					<execution>
						<id>attach-sources</id>
						<goals>
							<goal>jar-no-fork</goal>
						</goals>
					</execution>
				</executions>
				<configuration>
					<excludes>
						<exclude>log4j.properties</exclude> <!-- Logging configuration source should not be distributed. -->
					</excludes>
				</configuration>
			</plugin>

			<!-- Builds executable jar that includes all runtime dependencies -->
			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-assembly-plugin</artifactId>
				<version>3.1.1</version>
				<configuration>
					<descriptorRefs>
						<descriptorRef>jar-with-dependencies</descriptorRef>
					</descriptorRefs>
					<archive>
						<manifest>
							<mainClass>io.smartdatalake.app.LocalSmartDataLakeBuilder</mainClass>
						</manifest>
						<manifestEntries>
							<Implementation-Version>${project.version}</Implementation-Version>
						</manifestEntries>
					</archive>
					<skipAssembly>${skip.assembly}</skipAssembly>
				</configuration>
				<executions>
					<execution>
						<id>make-assembly</id>
						<phase>package</phase>
						<goals>
							<goal>single</goal>
						</goals>
					</execution>
				</executions>
			</plugin>

			<!-- Executes units tests with scalatest  -->
			<plugin>
				<groupId>org.scalatest</groupId>
				<artifactId>scalatest-maven-plugin</artifactId>
				<version>2.0.0</version>
				<configuration>
					<reportsDirectory>
						${project.build.directory}/scalatest-reports
					</reportsDirectory>
					<junitxml>.</junitxml>
					<filereports>
						${project.artifactId}.txt
					</filereports>
					<stdout>WT</stdout>  <!-- without color, show reminder of failed and canceled tests with short stack traces, see: http://www.scalatest.org/user_guide/using_scalatest_with_sbt-->
					<environmentVariables>
						<SPARK_LOCAL_IP>127.0.0.1</SPARK_LOCAL_IP> <!-- Suppresses Spark IP discovery during tests (when executed with mvn test) -->
					</environmentVariables>
				</configuration>
				<executions>
					<execution>
						<goals>
							<goal>test</goal>
						</goals>
					</execution>
				</executions>
			</plugin>

			<!-- Allows handling of version numbers -->
			<plugin>
				<groupId>org.codehaus.mojo</groupId>
				<artifactId>versions-maven-plugin</artifactId>
				<version>2.7</version>
			</plugin>

			<!-- Checks for declared but unused and undeclared but used dependencies in the verify stage -->
			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-dependency-plugin</artifactId>
				<version>3.1.1</version>
				<configuration>
					<failOnWarning>false</failOnWarning>
					<ignoreNonCompile>true</ignoreNonCompile>
					<ignoredUsedUndeclaredDependencies>
						<dependency>org.scalacheck:scalacheck_${scala.minor.version}</dependency>
					</ignoredUsedUndeclaredDependencies>
				</configuration>
				<executions>
					<execution>
						<goals>
							<goal>analyze-only</goal>
						</goals>
					</execution>
				</executions>
			</plugin>

			<!-- check for dependency version conflicts -->
			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-enforcer-plugin</artifactId>
				<version>3.0.0-M3</version>
				<configuration>
					<rules>
						<requireMavenVersion>
							<version>2.0.6</version>
						</requireMavenVersion>
						<requireJavaVersion>
							<version>1.8</version>
						</requireJavaVersion>
						<dependencyConvergence/>
						<requireProperty>
							<property>scala.minor.version</property>
							<message>You must setscala.minor.version property by activating profile scala-2.11 or scala-2.12!</message>
						</requireProperty>
					</rules>
				</configuration>
			</plugin>

		</plugins>

	</build>

	<!-- cleanup version of conflicting transitive dependencies reported by mvn enforcer:enforce -->
	<dependencyManagement>
		<dependencies>
			<!-- import of SPARK-PARENT pom dependency management to cleanup spark dependencies -->
			<dependency>
				<groupId>org.apache.spark</groupId>
				<artifactId>spark-parent_${scala.minor.version}</artifactId>
				<version>${spark.version}</version>
				<type>pom</type>
				<scope>import</scope>
			</dependency>
			<dependency>
				<groupId>commons-logging</groupId>
				<artifactId>commons-logging</artifactId>
				<version>1.1.3</version>
			</dependency>
			<dependency>
				<groupId>org.apache.commons</groupId>
				<artifactId>commons-compress</artifactId>
				<version>1.4.1</version>
			</dependency>
			<dependency>
				<groupId>io.swagger</groupId>
				<artifactId>swagger-annotations</artifactId>
				<version>1.5.22</version>
			</dependency>
			<dependency>
				<groupId>org.antlr</groupId>
				<artifactId>ST4</artifactId>
				<version>4.0.4</version>
			</dependency>
			<dependency>
				<groupId>org.antlr</groupId>
				<artifactId>antlr-runtime</artifactId>
				<version>3.4</version>
			</dependency>
			<dependency>
				<groupId>org.tukaani</groupId>
				<artifactId>xz</artifactId>
				<version>1.5</version>
			</dependency>
			<dependency>
				<groupId>org.scala-lang.modules</groupId>
				<artifactId>scala-xml_${scala.minor.version}</artifactId>
				<version>1.0.6</version>
			</dependency>
		</dependencies>
	</dependencyManagement>

	<dependencies>
		<dependency>
			<groupId>com.databricks</groupId>
			<artifactId>dbutils-api_2.11</artifactId>
			<version>0.0.4</version>
		</dependency>
		<dependency>
			<groupId>org.slf4j</groupId>
			<artifactId>slf4j-api</artifactId>
		</dependency>
		<dependency>
			<groupId>com.github.scopt</groupId>
			<artifactId>scopt_${scala.minor.version}</artifactId>
			<version>${scopt.version}</version>
		</dependency>

		<!-- hadoop-common is an used undeclared dependency, but if we would explicitly list it we would need to duplicate the exclusions as in spark-parent.
		     If we don't declare it, we get the exclusion over import of spark-parent dependencyManagement, which is more valuable -->
		<!--dependency>
			<groupId>org.apache.hadoop</groupId>
			<artifactId>hadoop-common</artifactId>
			<version>${hadoop.version}</version>
		</dependency-->

		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-core_${scala.minor.version}</artifactId>
			<version>${spark.version}</version>
			<scope>${spark.deps.scope}</scope>
		</dependency>

		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-sql_${scala.minor.version}</artifactId>
			<version>${spark.version}</version>
			<scope>${spark.deps.scope}</scope>
		</dependency>

		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-hive_${scala.minor.version}</artifactId>
			<version>${spark.version}</version>
			<scope>${spark.deps.scope}</scope>
		</dependency>

		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-catalyst_${scala.minor.version}</artifactId>
			<version>${spark.version}</version>
			<scope>${spark.deps.scope}</scope>
		</dependency>

		<dependency>
			<groupId>io.smartdatalake</groupId>
			<artifactId>spark-extensions_${scala.minor.version}</artifactId>
			<version>1.0.0</version>
		</dependency>

		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-sql-kafka-0-10_${scala.minor.version}</artifactId>
			<version>${spark.version}</version>
			<scope>runtime</scope>
			<exclusions>
				<!-- replace kafka client with newer version -->
				<exclusion>
					<groupId>org.apache.kafka</groupId>
					<artifactId>kafka-clients</artifactId>
				</exclusion>
			</exclusions>
		</dependency>
		<dependency>
			<groupId>org.apache.kafka</groupId>
			<artifactId>kafka-clients</artifactId>
			<version>${kafka.version}</version>
		</dependency>
		<!-- this is for reading from kafka using confluent schema registry with Spark -->
		<!-- this is an unused declared dependency but needed -->
		<dependency>
			<groupId>io.confluent</groupId>
			<artifactId>kafka-schema-registry-client</artifactId>
			<version>5.4.1</version>
			<exclusions>
				<exclusion>
					<groupId>org.apache.kafka</groupId>
					<artifactId>kafka-clients</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.avro</groupId>
					<artifactId>avro</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.slf4j</groupId>
					<artifactId>slf4j-api</artifactId>
				</exclusion>
				<exclusion>
					<groupId>com.fasterxml.jackson.core</groupId>
					<artifactId>jackson-databind</artifactId>
				</exclusion>
			</exclusions>
		</dependency>

		<dependency>
			<groupId>com.databricks</groupId>
			<artifactId>spark-xml_${scala.minor.version}</artifactId>
			<version>${databricks.spark-xml.version}</version>
			<scope>runtime</scope>
		</dependency>

		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-avro_${scala.minor.version}</artifactId>
			<version>${spark.version}</version>
			<scope>${spark.deps.scope}</scope>
		</dependency>

		<!-- this needs to have compile scope, if it's runtime it doesnt compile anymore -->
		<!-- this is an unused declared dependency, but it's needed to override scope -->
		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-tags_${scala.minor.version}</artifactId>
			<version>${spark.version}</version>
			<scope>compile</scope>
		</dependency>

		<dependency>
			<groupId>commons-io</groupId>
			<artifactId>commons-io</artifactId>
			<version>${commons-io.version}</version>
		</dependency>
		<dependency>
			<groupId>com.healthmarketscience.jackcess</groupId>
			<artifactId>jackcess</artifactId>
			<version>2.1.11</version>
		</dependency>

		<dependency>
			<groupId>com.crealytics</groupId>
			<artifactId>spark-excel_${scala.minor.version}</artifactId>
			<version>${spark.excel.version}</version>
			<scope>runtime</scope>
			<exclusions>
				<exclusion>
					<groupId>com.ibm.icu</groupId>
					<artifactId>icu4j</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.scala-lang.modules</groupId>
					<artifactId>scala-xml_${scala.minor.version}</artifactId>
				</exclusion>
			</exclusions>
		</dependency>
		<dependency>
			<groupId>org.apache.poi</groupId>
			<artifactId>poi</artifactId>
			<version>${poi.version}</version>
			<scope>runtime</scope>
		</dependency>
		<dependency>
			<groupId>org.apache.poi</groupId>
			<artifactId>poi-ooxml</artifactId>
			<version>${poi.version}</version>
			<scope>runtime</scope>
		</dependency>

		<dependency>
			<groupId>net.sf.ucanaccess</groupId>
			<artifactId>ucanaccess</artifactId>
			<version>${ucanaccess.version}</version>
			<scope>runtime</scope>
		</dependency>

		<dependency>
			<groupId>com.hierynomus</groupId>
			<artifactId>sshj</artifactId>
			<version>${sshj.version}</version>
		</dependency>

		<dependency>
			<groupId>org.scala-lang</groupId>
			<artifactId>scala-library</artifactId>
			<version>${scala.version}</version>
			<scope>${scala.deps.scope}</scope>
		</dependency>
		<dependency>
			<groupId>org.scala-lang</groupId>
			<artifactId>scala-reflect</artifactId>
			<version>${scala.version}</version>
			<scope>${scala.deps.scope}</scope>
		</dependency>
		<dependency>
			<groupId>org.scala-lang</groupId>
			<artifactId>scala-compiler</artifactId>
			<version>${scala.version}</version>
			<scope>${scala.deps.scope}</scope>
		</dependency>
		<dependency>
			<groupId>org.scala-lang.modules</groupId>
			<artifactId>scala-xml_${scala.minor.version}</artifactId>
			<scope>${scala.deps.scope}</scope>
		</dependency>

		<dependency>
			<groupId>com.typesafe</groupId>
			<artifactId>config</artifactId>
			<version>${typesafe.config.version}</version>
		</dependency>

		<dependency>
			<groupId>com.jsuereth</groupId>
			<artifactId>scala-arm_${scala.minor.version}</artifactId>
			<version>${scala-arm.version}</version>
		</dependency>

		<dependency>
			<groupId>com.splunk</groupId>
			<artifactId>splunk</artifactId>
			<version>${splunk.version}</version>
		</dependency>

		<dependency>
			<groupId>org.scalaj</groupId>
			<artifactId>scalaj-http_${scala.minor.version}</artifactId>
			<version>2.3.0</version>
		</dependency>

		<dependency>
			<groupId>javax.jms</groupId>
			<artifactId>jms</artifactId>
			<version>1.1</version>
		</dependency>

		<!-- Start Keycloak dependencies -->
		<dependency>
			<groupId>org.keycloak</groupId>
			<artifactId>keycloak-core</artifactId>
			<version>${keycloak.version}</version>
		</dependency>
		<dependency>
			<groupId>org.keycloak</groupId>
			<artifactId>keycloak-admin-client</artifactId>
			<version>${keycloak.version}</version>
		</dependency>

		<!--  only used for keycloak admin api -->
		<dependency>
			<groupId>org.jboss.resteasy</groupId>
			<artifactId>resteasy-client</artifactId>
			<version>3.1.3.Final</version>
			<scope>runtime</scope>
		</dependency>
		<!-- End Keycloak dependencies -->

		<dependency>
			<groupId>com.github.kxbmap</groupId>
			<artifactId>configs_${scala.minor.version}</artifactId>
			<version>0.4.4</version>
			<exclusions>
				<exclusion>
					<groupId>com.typesafe</groupId>
					<artifactId>config</artifactId>
				</exclusion>
			</exclusions>
		</dependency>

		<dependency>
			<groupId>org.apache.kafka</groupId>
			<artifactId>kafka_${scala.minor.version}</artifactId>
			<version>${kafka.version}</version>
			<exclusions>
				<exclusion>
					<groupId>commons-cli</groupId>
					<artifactId>commons-cli</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.scala-lang.modules</groupId>
					<artifactId>scala-java8-compat_${scala.minor.version}</artifactId>
				</exclusion>
			</exclusions>
		</dependency>

		<dependency>
			<groupId>io.monix</groupId>
			<artifactId>monix-eval_${scala.minor.version}</artifactId>
			<version>${monix.version}</version>
		</dependency>
		<dependency>
			<groupId>io.monix</groupId>
			<artifactId>monix-execution_${scala.minor.version}</artifactId>
			<version>${monix.version}</version>
		</dependency>

		<dependency>
			<groupId>com.github.mutcianm</groupId>
			<artifactId>ascii-graphs_${scala.minor.version}</artifactId>
			<version>0.0.6</version>
		</dependency>

		<dependency>
			<groupId>org.apache.commons</groupId>
			<artifactId>commons-pool2</artifactId>
			<version>2.8.0</version>
		</dependency>

		<dependency>
			<groupId>io.delta</groupId>
			<artifactId>delta-core_${scala.minor.version}</artifactId>
			<version>${deltaTable.version}</version>
		</dependency>


		<!-- TEST dependencies -->
		<dependency>
			<groupId>org.scalatest</groupId>
			<artifactId>scalatest_${scala.minor.version}</artifactId>
			<version>${scalatest.test.version}</version>
			<scope>test</scope>
			<exclusions>
				<exclusion>
					<groupId>org.scala-lang.modules</groupId>
					<artifactId>scala-xml_${scala.minor.version}</artifactId>
				</exclusion>
			</exclusions>
		</dependency>
		<dependency>
			<groupId>org.scalactic</groupId>
			<artifactId>scalactic_${scala.minor.version}</artifactId>
			<version>${scalatest.test.version}</version>
			<scope>test</scope>
		</dependency>
		<dependency>
			<groupId>org.scalacheck</groupId>
			<artifactId>scalacheck_${scala.minor.version}</artifactId>
			<version>1.14.3</version>
			<scope>test</scope>
		</dependency>
		<!-- Log4j is used in some unit tests to disable some exception logging -->
		<dependency>
			<groupId>log4j</groupId>
			<artifactId>log4j</artifactId>
			<scope>test</scope>
		</dependency>

		<dependency>
			<groupId>org.apache.sshd</groupId>
			<artifactId>sshd-sftp</artifactId>
			<version>${sshd.test.version}</version>
			<scope>test</scope>
		</dependency>
		<dependency>
			<groupId>org.apache.sshd</groupId>
			<artifactId>sshd-common</artifactId>
			<version>${sshd.test.version}</version>
			<scope>test</scope>
		</dependency>
		<dependency>
			<groupId>org.apache.sshd</groupId>
			<artifactId>sshd-core</artifactId>
			<version>${sshd.test.version}</version>
			<scope>test</scope>
		</dependency>
		<dependency>
			<groupId>com.github.tomakehurst</groupId>
			<artifactId>wiremock-standalone</artifactId>
			<version>2.25.1</version>
			<scope>test</scope>
		</dependency>

		<dependency>
			<groupId>net.i2p.crypto</groupId>
			<artifactId>eddsa</artifactId>
			<version>0.1.0</version>
			<scope>test</scope>
		</dependency>

		<!-- https://mvnrepository.com/artifact/net.manub/scalatest-embedded-kafka -->
		<!--- This is old and unmaintained, but supports up to kafka 0.10 which is what
		 the spark libraries are based on and also Scala 2.11. There is a replacement which we should
		 use once 2.11 is dead, including Schema Registry-->
		<dependency>
			<groupId>io.github.embeddedkafka</groupId>
			<artifactId>embedded-kafka_${scala.minor.version}</artifactId>
			<version>${kafka.version}</version>
			<scope>test</scope>
		</dependency>

	</dependencies>
</project>
